# -*- coding: utf-8 -*-
"""CreditCard (3) (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sESTNqtT0faXIWE1sXILWjeCGPHpV45U
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
#warnings.simplefilter(action='ignore', category = FutureWarning

import sklearn
import scipy as sc
import matplotlib as mpl
import itertools
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
import matplotlib.cm as cm

import itertools # advanced tools
from sklearn.preprocessing import StandardScaler # data normalization
from sklearn.model_selection import train_test_split # data split
from sklearn.metrics import roc_auc_score,roc_curve

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier # Decision tree algorithm
from sklearn.ensemble import RandomForestClassifier # Random forest tree algorithm
from sklearn.linear_model import LogisticRegression # Logistic regression algorithm
from sklearn.neighbors import KNeighborsClassifier # KNN algorithm
from sklearn.svm import SVC # SVM algorithm
from xgboost import XGBClassifier # XGBoost algorithm
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler 
from sklearn.metrics import accuracy_score # evaluation metric
from sklearn.metrics import f1_score # evaluation metric
from sklearn.metrics import confusion_matrix # evaluation metric
from sklearn.metrics import recall_score,precision_score
from sklearn.metrics import classification_report

import sklearn
import scipy as sc
import matplotlib as mpl
import itertools
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
import matplotlib.cm as cm

from sklearn.metrics import silhouette_score, silhouette_samples
from sklearn.cluster import KMeans
from imblearn.over_sampling import RandomOverSampler

import tensorflow as tf
import keras
from tensorflow.keras.layers import Dense,Activation,BatchNormalization,Dropout,Input,Multiply,Concatenate,Add,Conv1D
from tensorflow.keras.models import Sequential,Model
tf.random.set_seed(100)

from google.colab import drive
drive.mount('/content/drive')

creditcard =pd.read_csv('/content/drive/MyDrive/CreditCard/CreditCard/creditcard.csv')

#creditcard=creditcard.sample(10000)

creditcard.shape

#creditcard =pd.read_csv('creditcard.csv')

creditcard=creditcard.iloc[0:50000,:]

creditcard.shape

print(" counts of label '1': {}".format(sum(creditcard.Class == 1)))
print(" counts of label '0': {}".format(sum(creditcard.Class == 0)))

creditcard.head()

creditcard.shape

cases = len(creditcard)
nonfraud_count = len(creditcard[creditcard.Class == 0])
fraud_count = len(creditcard[creditcard.Class == 1])
fraud_percentage = round(fraud_count/nonfraud_count*100, 3)

print('CASE COUNT')
print('--------------------------------------------')
print('Total number of cases are {}'.format(cases))
print('Number of Non-fraud cases are {}'.format(nonfraud_count))
print('Number of fraud cases are {}'.format(fraud_count))
print('Percentage of fraud cases is {}'.format(fraud_percentage))
print('--------------------------------------------')

nonfraud_cases = creditcard[creditcard.Class == 0]
fraud_cases = creditcard[creditcard.Class == 1]

print('CASE AMOUNT STATISTICS')
print('--------------------------------------------')
print('NON-FRAUD CASE AMOUNT STATS')
print(nonfraud_cases.Amount.describe())
print('--------------------------------------------')
print('FRAUD CASE AMOUNT STATS')
print(fraud_cases.Amount.describe())
print('--------------------------------------------')

"""**EDA Analysis**

1) Target Distribution For Classification
"""

def target_classification(df, target):
    fig, ax = plt.subplots(figsize = (6, 6))
    
    labels = df[target].value_counts().index.tolist()
    palette = ["#0EB8F1", "#F1480F", "#971194", "#FEE715", "#101820"]

    ax.pie(df[target].value_counts(), labels = labels, autopct = '%1.2f%%', 
           startangle = 180, colors = palette[: len(labels)])

    ax.set_title(target)
    plt.show()

target_classification(creditcard, "Class")

"""2) Plotting Target with Two Features"""

def two_feature_classification(df, target, f1, f2):
    
    fig, ax = plt.subplots(figsize=(15, 8))
    ax.set_facecolor("#393838")

    X = df.drop(target, axis = 1)
    y = df[target].values
    
    labels = df[target].value_counts().index.tolist()

    ax.scatter(X.loc[y == 0, f1], X.loc[y == 0, f2], label = labels[0], alpha = 1, linewidth = 0, c = "#0EB8F1")
    ax.scatter(X.loc[y == 1, f1], X.loc[y == 1, f2], label = labels[1], alpha = 1, linewidth = 0, c = '#F1480F', marker = "X")

    ax.set_title("Distribution of " + target + " w.r.t " + f1 + " and " + f2)
    ax.set_xlabel(f1); ax.set_ylabel(f2)
    ax.legend()
    sns.despine(top = True, right = True, left = True, bottom = True)
    plt.show()

two_feature_classification(creditcard, "Class", "V1", "V2")

two_feature_classification(creditcard, "Class", "V3", "V4")

"""3) Target Distribution"""

fig = plt.figure(figsize=(16,12))
sns.distplot(creditcard['Amount']);
plt.show()

def feature_distribution(df, col):
    
    from scipy import stats
    
    skewness = np.round(df[col].skew(), 3)
    kurtosis = np.round(df[col].kurtosis(), 3)

    fig, axes = plt.subplots(1, 3, figsize = (21, 7))
    
    sns.kdeplot(data = df, x = col, fill = True, ax = axes[0], color = "#603F83", linewidth = 2)
    sns.boxplot(data = df, y = col, ax = axes[1], color = "#603F83",
                linewidth = 2, flierprops = dict(marker = "x", markersize = 3.5))
    stats.probplot(df[col], plot = axes[2])

    axes[0].set_title("Distribution \nSkewness: " + str(skewness) + "\nKurtosis: " + str(kurtosis))
    axes[1].set_title("Boxplot")
    axes[2].set_title("Probability Plot")
    fig.suptitle("For Feature:  " + col)
    
    for ax in axes:
        ax.set_facecolor("#C7D3D4FF")
        ax.grid(linewidth = 0.1)
    
    axes[2].get_lines()[0].set_markerfacecolor('#8157AE')
    axes[2].get_lines()[0].set_markeredgecolor('#603F83')
    axes[2].get_lines()[0].set_markeredgewidth(0.1)
    axes[2].get_lines()[1].set_color('#F1480F')
    axes[2].get_lines()[1].set_linewidth(3)
    
    sns.despine(top = True, right = True, left = True, bottom = True)
    plt.show()

feature_distribution(creditcard, "Amount")

"""4) Numerical Variable - Categorical Target"""

def feature_dist_clas(df, col, hue):
    
    fig, axes = plt.subplots(1, 4, figsize = (25, 5))
    order = sorted(df[hue].unique())
    palette = ["#0EB8F1", "#F1480F", "#971194", "#FEE715", "#101820"]
    
    sns.histplot(x = col, hue = hue, data = df, ax = axes[0], palette = palette[: df[hue].nunique()], edgecolor="black", linewidth=0.5)
    sns.kdeplot(x = col, hue = hue, data = df, fill = True, ax = axes[1], palette = palette[: df[hue].nunique()], linewidth = 2)
    sns.boxplot(y = col, hue = hue, data = df, x = [""] * len(df), ax = axes[2], 
                palette = palette[:len(order)], linewidth = 2, flierprops = dict(marker = "x", markersize = 3.5))
    
    sns.violinplot(y = col, hue = hue, data = df, x = [""] * len(df), ax = axes[3], palette = palette[: df[hue].nunique()])
    
    fig.suptitle("For Feature:  " + col)
    axes[0].set_title("Histogram For Feature " + col)
    axes[1].set_title("KDE Plot For Feature " + col)   
    axes[2].set_title("Boxplot For Feature " + col)   
    axes[3].set_title("Violinplot For Feature " + col)   
    
    for ax in axes:
        ax.set_facecolor("#C7D3D4FF")
        ax.grid(linewidth = 0.1)

feature_dist_clas(creditcard, "V1", "Class")

feature_dist_clas(creditcard, "V2", "Class")

feature_dist_clas(creditcard, "V3", "Class")

feature_dist_clas(creditcard, "V4", "Class")

feature_dist_clas(creditcard, "V5", "Class")

feature_dist_clas(creditcard, "V6", "Class")

feature_dist_clas(creditcard, "V7", "Class")

feature_dist_clas(creditcard, "V8", "Class")

"""5) Numerical Variable - Numerical Target"""

def plot_scatter(df, col, target):
    
    corr = df[[col, target]].corr()[col][1]    
    c = ["#EB0000"] if corr >= 0.7 else (["#800000"] if corr >= 0.3 else\
                                    (["#FF6363"] if corr >= 0 else\
                                    (["#000EAA"] if corr <= -0.7 else\
                                    (["#3845D3"] if corr <= -0.3 else ["#6CAAFA"]))))    

    fig, ax = plt.subplots(figsize = (10, 6))
    ax.set_facecolor("#C7D3D4FF")
    ax.grid(linewidth = 0.1)
    
    sns.scatterplot(x = col, y = target, data = df, c = c, ax = ax, edgecolor = "black")        
    ax.set_title("Correlation between " + col + " and " + target + " is: " + str(corr.round(4)))

plot_scatter(creditcard, "V1", "Amount")

"""6) Heatmap"""

def heatmap(df):
    
    fig, ax = plt.subplots(figsize = (15, 15))
    
    sns.heatmap(df.corr(), cmap = "coolwarm", annot = True, fmt = ".2f", annot_kws = {"fontsize": 9},
                vmin = -1, vmax = 1, square = True, linewidths = 0.01, linecolor = "black", cbar = False)
    
    sns.despine(top = True, right = True, left = True, bottom = True)

heatmap(creditcard)

heatmap(creditcard[creditcard["Class"] == 1])

heatmap(creditcard[creditcard["Class"] == 0])

plt.figure(figsize=(18,12))
plot = sns.distplot(a=creditcard["Time"], kde=True, color='green')
plot.set(xlabel ='Time', ylabel ='Frequency')
plt.show()

fraudulent_transactions = creditcard[creditcard['Class'] == 1]
nonfraudulent_transactions =creditcard[creditcard['Class'] == 0]

plt.figure(figsize=(18,12))

sns.distplot(a=nonfraudulent_transactions["Time"], kde=True)
plot = sns.distplot(a=fraudulent_transactions["Time"], kde=True)

plot.set(xlabel ='Time', ylabel ='Frequency')
plot.legend(['Not Fraud', 'Fraud'])
plt.show()

fig, axes = plt.subplots(8,4, figsize=(20, 12))
axes = axes.flatten()

for ax,i in zip(axes,creditcard.iloc[:30].columns):
    sns.kdeplot(data= creditcard.iloc[:30][i], ax=ax, fill=True,warn_singular=False)
plt.show()

"""**OUTLIER / ANOMALY DETECTION**"""

s,ax = plt.subplots(8,4,figsize = (12,30))
ax = ax.flatten()
for i,ax in zip(creditcard.iloc[:,:30].columns,ax):
      sns.boxplot(data= creditcard.iloc[:30][i], ax=ax)
plt.show()

"""**Feature Reduction**"""

X=creditcard

import sklearn
from sklearn.decomposition import PCA
pca = sklearn.decomposition.PCA(random_state=58)
pca.fit(X)

pca_expl_var = pd.DataFrame(pca.explained_variance_ratio_).reset_index()
pca_expl_var.columns = ["PC", "expl_var"]
#pca_expl_var.head()

fig, ax = plt.subplots()
fig.set_size_inches(12, 4, forward=True)

plt.title('Explained Variance for Principal Components of Ulabox Orders') 
plt.plot(pca_expl_var.PC, pca_expl_var.expl_var, color = 'tab:red')

ax.set_xlabel('Principal Component')
ax.set_ylabel('Explained Variance')
ax.set_xticks(np.arange(0, 12, step=1))
#plt.xticks(np.arange(0, n_clusters, step=1))
#plt.ylim(0, 0.2) 

plt.show()

pca = PCA(n_components=12)
pca.fit(X.values)

def pca_results(pca, df):
    dimensions = ['dimention {}'.format(i) for i in range(1,pca.n_components_+1)]
    fig, ax = plt.subplots(figsize=(18,12))
    components = pd.DataFrame(pca.components_)
    components.plot(ax=ax, kind='bar');
    labels = [str(s) for s in df.columns]
    ax.legend(labels)
    ax.set_ylabel('Feature Weights')
    ax.set_xticklabels(dimensions, rotation=90)
    for i, ev in enumerate(np.round(pca.explained_variance_ratio_, 3)):
        ax.text(i-0.04, ax.get_ylim()[1]+0.05, ev)
    plt.show()

def pca_2d_plot(pca, df):
    fig = plt.figure(figsize=(10,10))
    transformed_data = pca.transform(df.values)
    data = pd.DataFrame(transformed_data, columns=['dim'+str(i) for i in range(1,13)])
    sns.lmplot(x='dim1', y='dim2', data=data, size=12, fit_reg=False, scatter_kws={'s':8});
    sns.lmplot(x='dim3', y='dim4', data=data, size=12, fit_reg=False, scatter_kws={'s':8});
    plt.show()

pca_results(pca, X)

pca_2d_plot(pca, X)

pca_results(pca, X)



"""**Preprocess**"""

creditcard.isnull().sum()

#plot missing data before preproceesing
#x_na = (creditcard.isnull().sum() / len(creditcard)) * 100
#x_na = x_na.drop(x_na[x_na == 0].index).sort_values(ascending=False)[:80]
#missing_data = pd.DataFrame({'Missing Ratio' :x_na})
#missing_data.head(80)
#f, ax = plt.subplots(figsize=(15, 12))
#plt.xticks(rotation='90', fontsize=15)
#sns.barplot(x=x_na.index, y=x_na)
#plt.xlabel('Features', fontsize=20)
#plt.ylabel('Percent of missing values', fontsize=20)
#plt.title('Percent missing data by feature', fontsize=20)

def turkey_outlier_detector(df, cols=None):
    if cols  is None:
        cols = [str(s) for s in df.describe().columns]
        
    q1 = {}
    q3 = {}
    iqd = {}
    r_limit = {}
    l_limit = {}
    outlier_count = {}
    outlier_indices = {}
    for col in cols:
        q1[col] = np.percentile(df[col].values, 25)
        q3[col] = np.percentile(df[col].values, 75)
        iqd[col] = q3[col] - q1[col]
        r_limit[col] = q3[col] + 1.5*iqd[col]
        l_limit[col] = q1[col] - 1.5*iqd[col]
        data_outlier = df[~((df[col]<r_limit[col]).multiply(df[col]>l_limit[col]))]
        outlier_count[col] = data_outlier.shape[0]
        outlier_indices[col] = data_outlier.index
        
    for col in cols:
        print('_'*25)
        print(col+'-'*8+'>'+str(outlier_count[col]))
        
    return outlier_indices

outlier_indices = turkey_outlier_detector(creditcard)

#creditcard.drop(outlier_indices['Class'], inplace=True)

creditcard.describe()

"""**Feature Reduction**"""



"""

```
# This is formatted as code
```

**Feature Selection & Data Split**"""

# IMPORTING PACKAGES

import pandas as pd # data processing
import numpy as np # working with arrays
import matplotlib.pyplot as plt # visualization
#from termcolor import colored as cl # text customization
import itertools # advanced tools
import seaborn as sns

from sklearn.preprocessing import StandardScaler # data normalization
from sklearn.model_selection import train_test_split # data split

from sklearn.metrics import roc_auc_score,roc_curve

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier # Decision tree algorithm
from sklearn.ensemble import RandomForestClassifier # Random forest tree algorithm
from sklearn.linear_model import LogisticRegression # Logistic regression algorithm
from sklearn.neighbors import KNeighborsClassifier # KNN algorithm
from sklearn.svm import SVC # SVM algorithm
from xgboost import XGBClassifier # XGBoost algorithm
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler 
from sklearn.metrics import accuracy_score # evaluation metric
from sklearn.metrics import f1_score # evaluation metric
from sklearn.metrics import confusion_matrix # evaluation metric
from sklearn.metrics import recall_score,precision_score
from sklearn.metrics import classification_report

"""**Data Spliting**"""

# DATA SPLIT

X = creditcard.drop('Class', axis = 1).values
y = creditcard['Class'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

print('X_train samples : ', X_train[:1])
print('X_test samples : ',  X_test[0:1])
print('y_train samples : ', y_train[0:20])
print('y_test samples : ', y_test[0:20])

"""**modeling** """

# MODELING


# 1. Decision Tree

tree_model = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')
tree_model.fit(X_train, y_train)
tree_yhat = tree_model.predict(X_test)

# 2. KNN
knn=KNeighborsClassifier()
knn.fit(X_train, y_train)
knn_yhat = knn.predict(X_test)

# 3. Logistic Regression model
lr=LogisticRegression()
lr.fit(X_train, y_train)
lr_yhat = lr.predict(X_test)

#4. SVM
svm=SVC()
svm.fit(X_train, y_train)
svm_yhat = svm.predict(X_test)


# 5. Random Forest Tree

rf = RandomForestClassifier(max_depth = 4)
rf.fit(X_train, y_train)
rf_yhat = rf.predict(X_test)

"""**Evalution**"""

# 1. Accuracy score

print('ACCURACY SCORE')
print('------------------------------------------------------------------------',)
print('Accuracy score of the Decision Tree model is {}'.format(accuracy_score(y_test, tree_yhat)))
print('------------------------------------------------------------------------')
print('Accuracy score of the KNN model is {}'.format(accuracy_score(y_test, knn_yhat)))
print('------------------------------------------------------------------------')
print('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(y_test, lr_yhat)))
print('------------------------------------------------------------------------')
print('Accuracy score of the SVM model is {}'.format(accuracy_score(y_test, svm_yhat)))
print('------------------------------------------------------------------------')
print('Accuracy score of the Random Forest Tree model is {}'.format(accuracy_score(y_test, rf_yhat)))
print('------------------------------------------------------------------------')

# 2. F1 score

print('F1 SCORE')
print('------------------------------------------------------------------------')
print('F1 score of the Decision Tree model is {}'.format(f1_score(y_test, tree_yhat)))
print('------------------------------------------------------------------------')
print('F1 score of the KNN model is {}'.format(f1_score(y_test, knn_yhat)))
print('------------------------------------------------------------------------')
print('F1 score of the Logistic Regression model is {}'.format(f1_score(y_test, lr_yhat)))
print('------------------------------------------------------------------------')
print('F1 score of the SVM model is {}'.format(f1_score(y_test, svm_yhat)))
print('------------------------------------------------------------------------')
print('F1 score of the Random Forest Tree model is {}'.format(f1_score(y_test, rf_yhat)))
print('------------------------------------------------------------------------')

# 3. confusion matrix

tree_matrix = confusion_matrix(y_test, tree_yhat, labels = [0, 1]) # Decision Tree
knn_matrix = confusion_matrix(y_test, knn_yhat, labels = [0, 1]) # K-Nearest Neighbors
lr_matrix = confusion_matrix(y_test, lr_yhat, labels = [0, 1]) # Logistic Regression
svm_matrix = confusion_matrix(y_test, svm_yhat, labels = [0, 1]) # Support Vector Machine
rf_matrix = confusion_matrix(y_test, rf_yhat, labels = [0, 1]) # Random Forest Tree
print('------------------------------------------------------------------------')
print("tree_matrix")
print(tree_matrix)
print('------------------------------------------------------------------------')
print("knn_matrix")
print(knn_matrix)
print('------------------------------------------------------------------------')
print("lr_matrix")
print(lr_matrix)
print('------------------------------------------------------------------------')
print("svm_matrix")
print(svm_matrix)
print('------------------------------------------------------------------------')
print("rf_matrix")
print(rf_matrix)
print('------------------------------------------------------------------------')

# 4. Precision Score

print('Precision Score')
print('------------------------------------------------------------------------')
print('Precision score of the Decision Tree model is {}'.format(precision_score(y_test, tree_yhat)))
print('------------------------------------------------------------------------')
print('Precision score of the KNN model is {}'.format(precision_score(y_test, knn_yhat)))
print('------------------------------------------------------------------------')
print('Precision score of the Logistic Regression model is {}'.format(precision_score(y_test, lr_yhat)))
print('------------------------------------------------------------------------')
print('Precision score of the SVM model is {}'.format(precision_score(y_test, svm_yhat)))
print('------------------------------------------------------------------------')
print('Precision score of the Random Forest Tree model is {}'.format(precision_score(y_test, rf_yhat)))
print('------------------------------------------------------------------------')

# 5. Recall Score

print('Recall Score')
print('------------------------------------------------------------------------')
print('Recall score of the Decision Tree model is {}'.format(recall_score(y_test, tree_yhat)))
print('------------------------------------------------------------------------')
print('Recall score of the KNN model is {}'.format(recall_score(y_test, knn_yhat)))
print('------------------------------------------------------------------------')
print('Recall score of the Logistic Regression model is {}'.format(recall_score(y_test, lr_yhat)))
print('------------------------------------------------------------------------')
print('Recall score of the SVM model is {}'.format(recall_score(y_test, svm_yhat)))
print('------------------------------------------------------------------------')
print('Recall score of the Random Forest Tree model is {}'.format(recall_score(y_test, rf_yhat)))
print('------------------------------------------------------------------------')

# 6. classification report

print('classification report')
print('------------------------------------------------------------------------')
print('classification report of the Decision Tree model is {}'.format(classification_report(y_test, tree_yhat)))
print('------------------------------------------------------------------------')
print('classification reportof the KNN model is {}'.format(classification_report(y_test, knn_yhat)))
print('------------------------------------------------------------------------')
print('classification report of the Logistic Regression model is {}'.format(classification_report(y_test, lr_yhat)))
print('------------------------------------------------------------------------')
print('classification report of the SVM model is {}'.format(classification_report(y_test, svm_yhat)))
print('------------------------------------------------------------------------')
print('classification report of the Random Forest Tree model is {}'.format(classification_report(y_test, rf_yhat)))
print('------------------------------------------------------------------------')

"""**oversampling**"""

# OverSampling

print('After OverSampling, the shape of train_X: {}'.format(X_train_over.shape))
print('After OverSampling, the shape of train_y: {} \n'.format(y_train_over.shape))

print('After OverSampling, the shape of test_X: {}'.format(X_test_over.shape))
print('After OverSampling, the shape of test_y: {} \n'.format(y_test_over.shape))

print("After OverSampling, counts of label '1': {}".format(sum(y_train_over == 1)))
print("After OverSampling, counts of label '0': {}".format(sum(y_train_over == 0)))

print("After OverSampling, counts of label '1': {}".format(sum(y_test_over == 1)))
print("After OverSampling, counts of label '0': {}".format(sum(y_test_over == 0)))

"""**Modelling**"""

#1. Decision Tree 

tree_model = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')
tree_model.fit(X_train_over, y_train_over)
tree_over = tree_model.predict(X_test_over)

# 2. KNN
knn=KNeighborsClassifier()
knn.fit(X_train_over, y_train_over)
knn_over = knn.predict(X_test_over)

# 3. Logistic Regression model
lr=LogisticRegression()
lr.fit(X_train_over, y_train_over)
lr_over = lr.predict(X_test_over)

#4. SVM
svm=SVC()
svm.fit(X_train_over, y_train_over)
svm_over = svm.predict(X_test_over)


# 5. Random Forest Tree

rf = RandomForestClassifier(max_depth = 4)
rf.fit(X_train_over, y_train_over)
rf_over= rf.predict(X_test_over)

"""**Evalution**"""

# 1. Accuracy score

print('ACCURACY SCORE')
print('------------------------------------------------------------------------',)
print('Accuracy score of the Decision Tree model is {}'.format(accuracy_score(y_test_over, tree_over)))
print('------------------------------------------------------------------------')
print('Accuracy score of the KNN model is {}'.format(accuracy_score(y_test_over, knn_over)))
print('------------------------------------------------------------------------')
print('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(y_test_over, lr_over)))
print('------------------------------------------------------------------------')
print('Accuracy score of the SVM model is {}'.format(accuracy_score(y_test_over, svm_over)))
print('------------------------------------------------------------------------')
print('Accuracy score of the Random Forest Tree model is {}'.format(accuracy_score(y_test_over, rf_over)))
print('------------------------------------------------------------------------')

# 2. F1 score

print('F1 SCORE')
print('------------------------------------------------------------------------')
print('F1 score of the Decision Tree model is {}'.format(f1_score(y_test_over, tree_over)))
print('------------------------------------------------------------------------')
print('F1 score of the KNN model is {}'.format(f1_score(y_test_over, knn_over)))
print('------------------------------------------------------------------------')
print('F1 score of the Logistic Regression model is {}'.format(f1_score(y_test_over, lr_over)))
print('------------------------------------------------------------------------')
print('F1 score of the SVM model is {}'.format(f1_score(y_test_over, svm_over)))
print('------------------------------------------------------------------------')
print('F1 score of the Random Forest Tree model is {}'.format(f1_score(y_test_over, rf_over)))
print('------------------------------------------------------------------------')

# 4. Precision Score

print('Precision Score')
print('------------------------------------------------------------------------')
print('Precision score of the Decision Tree model is {}'.format(precision_score(y_test_over, tree_over)))
print('------------------------------------------------------------------------')
print('Precision score of the KNN model is {}'.format(precision_score(y_test_over, knn_over)))
print('------------------------------------------------------------------------')
print('Precision score of the Logistic Regression model is {}'.format(precision_score(y_test_over, lr_over)))
print('------------------------------------------------------------------------')
print('Precision score of the SVM model is {}'.format(precision_score(y_test_over, svm_over)))
print('------------------------------------------------------------------------')
print('Precision score of the Random Forest Tree model is {}'.format(precision_score(y_test_over, rf_over)))
print('------------------------------------------------------------------------')

# 5. Recall Score

print('Recall Score')
print('------------------------------------------------------------------------')
print('Recall score of the Decision Tree model is {}'.format(recall_score(y_test_over, tree_over)))
print('------------------------------------------------------------------------')
print('Recall score of the KNN model is {}'.format(recall_score(y_test_over, knn_over)))
print('------------------------------------------------------------------------')
print('Recall score of the Logistic Regression model is {}'.format(recall_score(y_test_over, lr_over)))
print('------------------------------------------------------------------------')
print('Recall score of the SVM model is {}'.format(recall_score(y_test_over, svm_over)))
print('------------------------------------------------------------------------')
print('Recall score of the Random Forest Tree model is {}'.format(recall_score(y_test_over, rf_over)))
print('------------------------------------------------------------------------')

# 1. Accuracy score

print('ACCURACY SCORE')
print('------------------------------------------------------------------------',)
print('Accuracy score of the Decision Tree model is {}'.format(accuracy_score(y_test_over, tree_over)))
print('------------------------------------------------------------------------')
print('Accuracy score of the KNN model is {}'.format(accuracy_score(y_test_over, knn_over)))
print('------------------------------------------------------------------------')
print('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(y_test_over, lr_over)))
print('------------------------------------------------------------------------')
print('Accuracy score of the SVM model is {}'.format(accuracy_score(y_test_over, svm_over)))
print('------------------------------------------------------------------------')
print('Accuracy score of the Random Forest Tree model is {}'.format(accuracy_score(y_test_over, rf_over)))
print('------------------------------------------------------------------------')

# 2. F1 score

print('F1 SCORE')
print('------------------------------------------------------------------------')
print('F1 score of the Decision Tree model is {}'.format(f1_score(y_test_over, tree_over)))
print('------------------------------------------------------------------------')
print('F1 score of the KNN model is {}'.format(f1_score(y_test_over, knn_over)))
print('------------------------------------------------------------------------')
print('F1 score of the Logistic Regression model is {}'.format(f1_score(y_test_over, lr_over)))
print('------------------------------------------------------------------------')
print('F1 score of the SVM model is {}'.format(f1_score(y_test_over, svm_over)))
print('------------------------------------------------------------------------')
print('F1 score of the Random Forest Tree model is {}'.format(f1_score(y_test_over, rf_over)))
print('------------------------------------------------------------------------')

# 3. confusion_matrix

tree_matrix = confusion_matrix(y_test_over, tree_over, labels = [0, 1]) # Decision Tree
knn_matrix = confusion_matrix(y_test_over, knn_over, labels = [0, 1]) # K-Nearest Neighbors
lr_matrix = confusion_matrix(y_test_over, lr_over, labels = [0, 1]) # Logistic Regression
svm_matrix = confusion_matrix(y_test_over, svm_over, labels = [0, 1]) # Support Vector Machine
rf_matrix = confusion_matrix(y_test_over, rf_over, labels = [0, 1]) # Random Forest Tree
print('------------------------------------------------------------------------')
print("tree_matrix")
print(tree_matrix)
print('------------------------------------------------------------------------')
print("knn_matrix")
print(knn_matrix)
print('------------------------------------------------------------------------')
print("lr_matrix")
print(lr_matrix)
print('------------------------------------------------------------------------')
print("svm_matrix")
print(svm_matrix)
print('------------------------------------------------------------------------')
print("rf_matrix")
print(rf_matrix)
print('------------------------------------------------------------------------')

# 4. Precision Score

print('Precision Score')
print('------------------------------------------------------------------------')
print('Precision score of the Decision Tree model is {}'.format(precision_score(y_test_over, tree_over)))
print('------------------------------------------------------------------------')
print('Precision score of the KNN model is {}'.format(precision_score(y_test_over, knn_over)))
print('------------------------------------------------------------------------')
print('Precision score of the Logistic Regression model is {}'.format(precision_score(y_test_over, lr_over)))
print('------------------------------------------------------------------------')
print('Precision score of the SVM model is {}'.format(precision_score(y_test_over, svm_over)))
print('------------------------------------------------------------------------')
print('Precision score of the Random Forest Tree model is {}'.format(precision_score(y_test_over, rf_over)))
print('------------------------------------------------------------------------')

# 5. Recall Score

print('Recall Score')
print('------------------------------------------------------------------------')
print('Recall score of the Decision Tree model is {}'.format(recall_score(y_test_over, tree_over)))
print('------------------------------------------------------------------------')
print('Recall score of the KNN model is {}'.format(recall_score(y_test_over, knn_over)))
print('------------------------------------------------------------------------')
print('Recall score of the Logistic Regression model is {}'.format(recall_score(y_test_over, lr_over)))
print('------------------------------------------------------------------------')
print('Recall score of the SVM model is {}'.format(recall_score(y_test_over, svm_over)))
print('------------------------------------------------------------------------')
print('Recall score of the Random Forest Tree model is {}'.format(recall_score(y_test_over, rf_over)))
print('------------------------------------------------------------------------')

# 6. classification report

print('classification report')
print('------------------------------------------------------------------------')
print('classification report of the Decision Tree model is {}'.format(classification_report(y_test_over, tree_over)))
print('------------------------------------------------------------------------')
print('classification reportof the KNN model is {}'.format(classification_report(y_test_over, knn_over)))
print('------------------------------------------------------------------------')
print('classification report of the Logistic Regression model is {}'.format(classification_report(y_test_over, lr_over)))
print('------------------------------------------------------------------------')
print('classification report of the SVM model is {}'.format(classification_report(y_test_over, svm_over)))
print('------------------------------------------------------------------------')
print('classification report of the Random Forest Tree model is {}'.format(classification_report(y_test_over, rf_over)))
print('------------------------------------------------------------------------')

"""**undersampling**"""

# under sampling

under_sampler = RandomUnderSampler()
X, y = under_sampler.fit_resample(X, y)

X_train_under, X_test_under, y_train_under, y_test_under = train_test_split(X, y, test_size = 0.2, random_state = 0)

print('After underSampling, the shape of train_X: {}'.format(X_train_under.shape))
print('After underSampling, the shape of train_y: {} \n'.format(y_train_under.shape))

print('After underSampling, the shape of test_X: {}'.format(X_test_under.shape))
print('After underSampling, the shape of test_y: {} \n'.format(y_test_under.shape))

print("After underSampling, counts of label '1': {}".format(sum(y_train_under == 1)))
print("After underSampling, counts of label '0': {}".format(sum(y_train_under == 0)))

print("After underSampling, counts of label '1': {}".format(sum(y_test_under == 1)))
print("After underSampling, counts of label '0': {}".format(sum(y_test_under == 0)))

"""**Modeling**"""

#1. Decision Tree 

tree_model = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')
tree_model.fit(X_train_under, y_train_under)
tree_under = tree_model.predict(X_test_under)

# 2. KNN
knn=KNeighborsClassifier()
knn.fit(X_train_under, y_train_under)
knn_under = knn.predict(X_test_under)

# 3. Logistic Regression model
lr=LogisticRegression()
lr.fit(X_train_under, y_train_under)
lr_under = lr.predict(X_test_under)

#4. SVM
svm=SVC()
svm.fit(X_train_under, y_train_under)
svm_under = svm.predict(X_test_under)


# 5. Random Forest Tree

rf = RandomForestClassifier(max_depth = 4)
rf.fit(X_train_under, y_train_under)
rf_under= rf.predict(X_test_under)

"""**Evalution**"""

# 1. Accuracy score

print('ACCURACY SCORE')
print('------------------------------------------------------------------------')
print('Accuracy score of the Decision Tree model is {}'.format(accuracy_score(y_test_under, tree_under)))
print('------------------------------------------------------------------------')
print('Accuracy score of the KNN model is {}'.format(accuracy_score(y_test_under, knn_under)))
print('------------------------------------------------------------------------')
print('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(y_test_under, lr_under)))
print('------------------------------------------------------------------------')
print('Accuracy score of the SVM model is {}'.format(accuracy_score(y_test_under, svm_under)))
print('------------------------------------------------------------------------')
print('Accuracy score of the Random Forest Tree model is {}'.format(accuracy_score(y_test_under, rf_under)))
print('------------------------------------------------------------------------')

# 2. F1 score

print('F1 SCORE')
print('------------------------------------------------------------------------')
print('F1 score of the Decision Tree model is {}'.format(f1_score(y_test_under, tree_under)))
print('------------------------------------------------------------------------')
print('F1 score of the KNN model is {}'.format(f1_score(y_test_under, knn_under)))
print('------------------------------------------------------------------------')
print('F1 score of the Logistic Regression model is {}'.format(f1_score(y_test_under, lr_under)))
print('------------------------------------------------------------------------')
print('F1 score of the SVM model is {}'.format(f1_score(y_test_under, svm_under)))
print('------------------------------------------------------------------------')
print('F1 score of the Random Forest Tree model is {}'.format(f1_score(y_test_under, rf_under)))
print('------------------------------------------------------------------------')

# 3. confusion_matrix

tree_matrix = confusion_matrix(y_test_under, tree_under, labels = [0, 1]) # Decision Tree
knn_matrix = confusion_matrix(y_test_under, knn_under, labels = [0, 1]) # K-Nearest Neighbors
lr_matrix = confusion_matrix(y_test_under, lr_under, labels = [0, 1]) # Logistic Regression
svm_matrix = confusion_matrix(y_test_under, svm_under, labels = [0, 1]) # Support Vector Machine
rf_matrix = confusion_matrix(y_test_under, rf_under, labels = [0, 1]) # Random Forest Tree
print('------------------------------------------------------------------------')
print("tree_matrix")
print(tree_matrix)
print('------------------------------------------------------------------------')
print("knn_matrix")
print(knn_matrix)
print('------------------------------------------------------------------------')
print("lr_matrix")
print(lr_matrix)
print('------------------------------------------------------------------------')
print("svm_matrix")
print(svm_matrix)
print('------------------------------------------------------------------------')
print("rf_matrix")
print(rf_matrix)
print('------------------------------------------------------------------------')

# 4. Precision Score

print('Precision Score')
print('------------------------------------------------------------------------')
print('Precision score of the Decision Tree model is {}'.format(precision_score(y_test_under, tree_under)))
print('------------------------------------------------------------------------')
print('Precision score of the KNN model is {}'.format(precision_score(y_test_under, knn_under)))
print('------------------------------------------------------------------------')
print('Precision score of the Logistic Regression model is {}'.format(precision_score(y_test_under, lr_under)))
print('------------------------------------------------------------------------')
print('Precision score of the SVM model is {}'.format(precision_score(y_test_under, svm_under)))
print('------------------------------------------------------------------------')
print('Precision score of the Random Forest Tree model is {}'.format(precision_score(y_test_under, rf_under)))
print('------------------------------------------------------------------------')

# 5. Recall Score

print('Recall Score')
print('------------------------------------------------------------------------')
print('Recall score of the Decision Tree model is {}'.format(recall_score(y_test_under, tree_under)))
print('------------------------------------------------------------------------')
print('Recall score of the KNN model is {}'.format(recall_score(y_test_under, knn_under)))
print('------------------------------------------------------------------------')
print('Recall score of the Logistic Regression model is {}'.format(recall_score(y_test_under, lr_under)))
print('------------------------------------------------------------------------')
print('Recall score of the SVM model is {}'.format(recall_score(y_test_under, svm_under)))
print('------------------------------------------------------------------------')
print('Recall score of the Random Forest Tree model is {}'.format(recall_score(y_test_under, rf_under)))
print('------------------------------------------------------------------------')

# 6. classification report

print('classification report')
print('------------------------------------------------------------------------')
print('classification report of the Decision Tree model is {}'.format(classification_report(y_test_under, tree_under)))
print('------------------------------------------------------------------------')
print('classification reportof the KNN model is {}'.format(classification_report(y_test_under, knn_under)))
print('------------------------------------------------------------------------')
print('classification report of the Logistic Regression model is {}'.format(classification_report(y_test_under, lr_under)))
print('------------------------------------------------------------------------')
print('classification report of the SVM model is {}'.format(classification_report(y_test_under, svm_under)))
print('------------------------------------------------------------------------')
print('classification report of the Random Forest Tree model is {}'.format(classification_report(y_test_under, rf_under)))
print('------------------------------------------------------------------------')

"""**Deep Learning (Bsaic Model)**"""

import tensorflow as tf
import keras
from tensorflow.keras.layers import Dense,Activation,BatchNormalization,Dropout,Input,Multiply,Concatenate,Add,Conv1D
from tensorflow.keras.models import Sequential,Model
tf.random.set_seed(100)

def basic_model():
    x_input = Input(shape=(X_train.shape[1],))
#     model.add(Conv1D(32, 2, activation=relu, input_shape = X_train[0].shape))
#     model.add(BatchNormalization())
#     model.add(Dropout(0.2))
    x1 = Dense(100,activation='relu' )(x_input)
    x2 = Dense(128, activation='relu')(x1)
    x3 = Dense(256,activation='relu' )(x2)
    z2 = Dense(100,activation='relu')(x3)
    z31 = BatchNormalization()(z2) 
    #z31 = Dropout(0.3)(z31)

    
    x4 =  Dense(128,activation='relu')(z31)
    x5 =  Dense(64,activation='relu')(x4)
    z32 = BatchNormalization()(x5)
    #z32 = Dropout(0.3)(z32)

    x6 =  Dense(32,activation='relu')(z32)
    
    x_output = Dense(units=1,activation='sigmoid')(x6)
    model = Model(inputs=x_input, outputs=x_output,name='Model')
    return model

model = basic_model()
model.summary()

from tensorflow.keras.utils import plot_model
plot_model(model,  to_file='TPS_Model.png', show_shapes=True,show_layer_names=True)

from sklearn.utils.class_weight import compute_class_weight
class_weights = compute_class_weight(class_weight = "balanced", classes= np.unique(y_train), y= y_train)
cw = dict(zip(np.unique(y_train), class_weights))
print(cw)

model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy']) 
model.fit(X_train,y_train, batch_size =100 , class_weight = cw , epochs=50, validation_data= (X_test,y_test))

pd.DataFrame(model.history.history).plot()

preds  = model.predict(X_test)
predictions = preds.copy()
predictions[predictions <= 0.8] = 0
predictions[predictions > 0.8] = 1

from sklearn.metrics import confusion_matrix , classification_report

cm = pd.DataFrame(data= confusion_matrix(y_test, predictions, labels=[0, 1]),index=["NO-FRAUD", "FRAUD"],
columns=["NO_FRAUD", "FRAUD"])
import seaborn as sns
sns.heatmap(cm,annot=True,fmt="d")

print(classification_report(y_test,predictions))

"""**Deep Learning (DroupOut)**"""

def Dropout_model():
    x_input = Input(shape=(X_train.shape[1],))
#     model.add(Conv1D(32, 2, activation=relu, input_shape = X_train[0].shape))
#     model.add(BatchNormalization())
#     model.add(Dropout(0.2))
    x1 = Dense(100,activation='relu' )(x_input)
    x2 = Dense(128, activation='relu')(x1)
    x3 = Dense(256,activation='relu' )(x2)
    z2 = Dense(100,activation='relu')(x3)
    z31 = BatchNormalization()(z2) 
    z31 = Dropout(0.3)(z31)

    
    x4 =  Dense(128,activation='relu')(z31)
    x5 =  Dense(64,activation='relu')(x4)
    z32 = BatchNormalization()(x5)
    z32 = Dropout(0.3)(z32)

    x6 =  Dense(32,activation='relu')(z32)
    
    x_output = Dense(units=1,activation='sigmoid')(x6)
    model = Model(inputs=x_input, outputs=x_output,name='Model')
    return model

model = Dropout_model()
model.summary()

from tensorflow.keras.utils import plot_model
plot_model(model,  to_file='TPS_Model.png', show_shapes=True,show_layer_names=True)

from sklearn.utils.class_weight import compute_class_weight
class_weights = compute_class_weight(class_weight = "balanced", classes= np.unique(y_train), y= y_train)
cw = dict(zip(np.unique(y_train), class_weights))
print(cw)

model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy']) 
model.fit(X_train,y_train, batch_size =100 , class_weight = cw , epochs=50, validation_data= (X_test,y_test))

pd.DataFrame(model.history.history).plot()

preds  = model.predict(X_test)
predictions = preds.copy()
predictions[predictions <= 0.8] = 0
predictions[predictions > 0.8] = 1

from sklearn.metrics import confusion_matrix , classification_report

cm = pd.DataFrame(data= confusion_matrix(y_test, predictions, labels=[0, 1]),index=["NO-FRAUD", "FRAUD"],
columns=["NO_FRAUD", "FRAUD"])
import seaborn as sns
sns.heatmap(cm,annot=True,fmt="d")

print(classification_report(y_test,predictions))

"""**Unsupervised Learning**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import scipy as sc
import matplotlib as mpl
import itertools
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
import matplotlib.cm as cm

from sklearn.metrics import silhouette_score, silhouette_samples

from sklearn.cluster import KMeans
from imblearn.over_sampling import RandomOverSampler

X = creditcard.drop('Class', axis = 1).values
y = creditcard['Class'].values

oversample = RandomOverSampler(sampling_strategy='minority')
X, y = oversample.fit_resample(X, y)

#X_train_over, X_test_over, y_train_over, y_test_over = train_test_split(X, y, test_size = 0.3, random_state = 0)

X,y

#find number of cluster
clusters = range(1,20)
inertia = []
for n in clusters:
    kmeans = KMeans(n_clusters=n)
    kmeans.fit(X)
    inertia.append(kmeans.inertia_)
    
fig, ax = plt.subplots(figsize=(10,8))
ax.plot(clusters, inertia);
plt.show()

#evaluate K-means models
k_values = [2, 3, 5]

sse_score = []
sil_score = []
for k in k_values:
    model = KMeans(n_clusters=k, random_state=58)
    km = model.fit(X)
    sse_score.append(km.inertia_)
    cluster_labels = km.fit_predict(X)
    silhouette_avg = silhouette_score(X, cluster_labels)
    sil_score.append(silhouette_avg)

    msg = "Clusters:%s Elbow:%f.2 Sil: %f.3" % (k, km.inertia_, silhouette_avg)
    print(msg)
    print(pd.Series(cluster_labels).value_counts())

# plot the K-means Elbow and Silhouette scores
k_values = [2, 3, 5]

fig, ax1 = plt.subplots()
ax1.plot(k_values, sse_score, color = 'tab:red')
ax1.set_xlabel('K')
# Make the y-axis label, ticks and tick labels match the line color.
ax1.set_ylabel('Elbow', color='tab:red')
ax1.tick_params('y', colors='tab:red')
ax1.set_xticks([2, 3, 5])

ax2 = ax1.twinx()
ax2.plot(k_values, sil_score, color = 'green')
ax2.set_ylabel('Silhouette Score', color='green')
ax2.tick_params('y', colors='green')

fig.tight_layout()
plt.show()

# Make silhouette plots
seed=58
from sklearn.cluster import KMeans

model_list = []
model_list.append((KMeans(n_clusters=2, random_state=seed)))
model_list.append((KMeans(n_clusters=3, random_state=seed)))
model_list.append((KMeans(n_clusters=5, random_state=seed)))
#model_list.append((KMeans(n_clusters=11, random_state=seed)))

score=[]

for model in model_list:
    # Create a subplot 
    fig, ax1 = plt.subplots()
    fig.set_size_inches(18, 6)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, .5]
    ax1.set_xlim([-0.3, .5])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    #ax1.set_ylim([0, X_pca_sample_pca.shape[0] + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value 
    clusterer =  model.fit(X) 
    cluster_labels = clusterer.labels_
    cluster_dist = pd.DataFrame(cluster_labels, columns=['Cluster'])
    
    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("The average silhouette_score is: ", silhouette_avg)
    msg = "Clusters:%s Elbow:%f.2 Sil: %f.3" % ((model.get_params()["n_clusters"]),
                                              clusterer.inertia_, silhouette_avg)
    print(msg)
    print(pd.Series(cluster_labels).value_counts())
    score.append(silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(model.get_params()["n_clusters"]):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) /model.get_params()["n_clusters"])
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title(model)
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.3, -0.1, 0, 0.2, 0.4, 0.6, ])

    #plt.suptitle(("Comparison of Clustering Models"), fontsize=14, fontweight='bold')

plt.show()

"""**Modeling**"""

#X=pd.DataFrame(data=X)
#y=pd.DataFrame(data=y)

"""y.rename(columns={0: "labels"})"""

k = 3
kmeans_3 = KMeans(n_clusters=3)
kmeans_3.fit(X)

frame=pd.DataFrame(data=X)
#farme=X

frame['Cluster'] = kmeans_3.predict(X)

X=np.c_[ X, y, kmeans_3.predict(X) ]

frame['labels']=y

frame

X

"""#df
frame.rename(columns={0: "labels"})
"""

sns.catplot(x="Cluster", y="labels", data=frame)

#sns.catplot(x="Cluster", y="labels", kind="swarm", data=frame)

sns.catplot(x="Cluster", y="labels", kind="box", data=frame)

#g = sns.catplot(x="Cluster", y="labels", kind="violin", inner=None, data=frame)
#sns.swarmplot(x="Cluster", y="labels", color="k", size=3, data=frame, ax=g.ax)

"""**Evaluation**"""

#frame[:,:-1] = 0

#Class 0
frame.loc[frame['Cluster']==0, 'class'] = 'X0'
X0=frame[frame['Cluster']==0]
y0=frame.loc[:,'labels'][frame['Cluster']==0]
X0.describe()

#Class 1
frame.loc[frame['Cluster']==1, 'class'] = 'X1'
X1=frame[frame['Cluster']==1]
y1=frame.loc[:,'labels'][frame['Cluster']==1]
X1.describe()

#Class 2
frame.loc[frame['Cluster']==2, 'class'] = 'X2'
X2=frame[frame['Cluster']==2]
y2=frame.loc[:,'labels'][frame['Cluster']==2]
X2.describe()

#Result
frame.to_csv('Clustring_Result.csv')

def pca_2d_plot_labels(pca, df, frame):
    plt.figure(figsize=(18,18));
    transformed_data = pca.transform(df.values)
    data = pd.DataFrame({'dim1':transformed_data[:,0], 'dim2':transformed_data[:,1], 'labels':frame['labels'].values})
    sns.lmplot(x='dim1',y='dim2',hue='labels',data=data, fit_reg=False, size=16);
    data1 = pd.DataFrame({'dim2':transformed_data[:,1], 'dim3':transformed_data[:,2], 'labels':frame['labels'].values})
    sns.lmplot(x='dim2',y='dim3',hue='labels',data=data1, fit_reg=False, size=16);
    plt.show()

pca_2d_plot_labels(pca, df, frame)

"""# Unsupervised-Supervised Modeling"""

X0_train, X0_test, y0_train, y0_test = train_test_split(X[:,:-2][X[:,-1] == 0], X[:,-2][X[:,-1] ==0 ], test_size = 0.2, random_state = 0)
X1_train, X1_test, y1_train, y1_test = train_test_split(X[:,:-2][X[:,-1] == 1], X[:,-2][X[:,-1] ==1 ], test_size = 0.2, random_state = 0)
X2_train, X2_test, y2_train, y2_test = train_test_split(X[:,:-2][X[:,-1] == 2], X[:,-2][X[:,-1] ==2 ], test_size = 0.2, random_state = 0)

'''
iteration=10
#Model1:  Decision Tree

def model(xtrain,xtest,ytrain,ytest):
    model = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')
    model.fit(xtrain,ytrain)
    y_hat_train=model.predict(xtrain)
    y_hat_test=model.predict(xtest)
    accuracy_score=accuracy_score(y_test, y_hat_test)
    f1_score=f1_score(y_test, y_hat_test)
    #confusion_matrix = confusion_matrix(y_test, y_hat_test, labels = [0, 1])
    precision_score=precision_score(y_test, y_hat_test)
    recall_score=recall_score(y_test, y_hat_test)
    #classification_report=classification_report(y_test, y_hat_test)
    return accuracy_score,f1_score, precision_score,recall_score,#confusion_matrix,classification_report
# X0
accuracy_score,f1_score, precision_score, recall_score=[],[],[],[]
for i in range(iteration):
    X0_train, X0_test, y0_train, y0_test = train_test_split(X[:,:-1][X[:,-1] == 0], X[:,-1][X[:,-1] ==0 ], test_size = 0.2, random_state = 0)
    output=model(X_train, X_test, y_train, y_test)
    accuracy_score.append(output[0])
    f1_score.append(output[1])
    precision_score.append(output[2])
    recall_score.append(output[3])
accuracy_score_X0,f1_score_X0,precision_score_X0, recall_score_X0=np.mean(accuracy_score),np.mean(f1_score),np.mean(precision_score),np.mean(recall_score)
print('method: X0','\n','accuracy_score for Decision Tree:',accuracy_score_X0,'\n','f1_score test for Decision Tree:',f1_score_X0,'\n','precision_score for Decision Tree  :', precision_score_X0,'\n','recall_score_X0 for Decision Tree  :', recall_score,'\n')

'''
#impute
r2_score_train,r2_score_test, MSE, MAE=[],[],[],[]
for i in range(iteration):
    X_train_impute, X_test_impute, y_train_impute, y_test_impute = train_test_split(all_data_impute.loc[price_impute.iloc[:,0]!=0,:],price_impute.loc[price_impute.iloc[:,0]!=0,:], test_size=0.1)
    output=model1(X_train_impute, X_test_impute, y_train_impute, y_test_impute)
    r2_score_train.append(output[0])
    r2_score_test.append(output[1])
    MSE.append(output[2])
    MAE.append(output[3])
r2_score_train_impute1,r2_score_test_impute1,MSE_impute1, MAE_impute1=np.mean(r2_score_train),np.mean(r2_score_test),np.mean(MSE),np.mean(MAE)
print('method: impute','\n','r2_score train for LinearRegression:',r2_score_train_impute1,'\n','r2_score test for LinearRegression:',r2_score_test_impute1,'\n','MSE for LinearRegression  :', MSE_impute1,'\n','MAE for LinearRegression  :', MAE_impute1,'\n')
# dropna_stepwise
r2_score_train,r2_score_test, MSE, MAE=[],[],[],[]
for i in range(iteration):
    X_train_dropna_stepwise, X_test_dropna_stepwise, y_train_dropna_stepwise, y_test_dropna_stepwise = train_test_split(all_data_dropna_stepwise.loc[price_dropna.iloc[:,0]!=0,:],price_dropna.loc[price_dropna.iloc[:,0]!=0,:], test_size=0.1)
    output=model1(X_train_dropna_stepwise, X_test_dropna_stepwise, y_train_dropna_stepwise, y_test_dropna_stepwise)
    r2_score_train.append(output[0])
    r2_score_test.append(output[1])
    MSE.append(output[2])
    MAE.append(output[3])
r2_score_train_dropna_stepwise1,r2_score_test_dropna_stepwise1,MSE_dropna_stepwise1, MAE_dropna_stepwise1=np.mean(r2_score_train),np.mean(r2_score_test),np.mean(MSE),np.mean(MAE)
print('method: dropna_stepwise','\n','r2_score train for LinearRegression:',r2_score_train_dropna_stepwise1,'\n','r2_score test for LinearRegression:',r2_score_test_dropna_stepwise1,'\n','MSE for LinearRegression  :', MSE_dropna_stepwise1,'\n','MAE for LinearRegression  :', MAE_dropna_stepwise1,'\n')
# impute_stepwise
r2_score_train,r2_score_test, MSE, MAE=[],[],[],[]
for i in range(iteration):
    X_train_impute_stepwise, X_test_impute_stepwise, y_train_impute_stepwise, y_test_impute_stepwise = train_test_split(all_data_impute_stepwise.loc[price_impute.iloc[:,0]!=0,:],price_impute.loc[price_impute.iloc[:,0]!=0,:], test_size=0.1)
    output=model1(X_train_impute_stepwise, X_test_impute_stepwise, y_train_impute_stepwise, y_test_impute_stepwise)
    r2_score_train.append(output[0])
    r2_score_test.append(output[1])
    MSE.append(output[2])
    MAE.append(output[3])
r2_score_train_impute_stepwise1,r2_score_test_impute_stepwise1,MSE_impute_stepwise1, MAE_impute_stepwise1=np.mean(r2_score_train),np.mean(r2_score_test),np.mean(MSE),np.mean(MAE)
print('method: impute_stepwise','\n','r2_score train for LinearRegression:',r2_score_train_impute_stepwise1,'\n','r2_score test for LinearRegression:',r2_score_test_impute_stepwise1,'\n','MSE for LinearRegression  :', MSE_impute_stepwise1,'\n','MAE for LinearRegression  :', MAE_impute_stepwise1,'\n')

#Model2: lasso Regression

'''
# 1. Decision Tree

tree_model = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')

#Model0
tree_model.fit(X0_train, y0_train)
tree_yhat0 = tree_model.predict(X0_test)

# MODELING


# 1. Decision Tree

tree_model = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')

#Model0
tree_model.fit(X0_train, y0_train)
tree_yhat0 = tree_model.predict(X0_test)

#Model1
tree_model.fit(X1_train, y1_train)
tree_yhat1 = tree_model.predict(X1_test)

#Model2
tree_model.fit(X2_train, y2_train)
tree_yhat2 = tree_model.predict(X2_test)

# 2. KNN
knn=KNeighborsClassifier()

#Model0
knn.fit(X0_train, y0_train)
knn_yhat0 = knn.predict(X0_test)

#Model1
knn.fit(X1_train, y1_train)
knn_yhat1 = knn.predict(X1_test)

#Model2
knn.fit(X2_train, y2_train)
knn_yhat2 = knn.predict(X2_test)


# 3. Logistic Regression model
lr=LogisticRegression()

#Model0
lr.fit(X0_train, y0_train)
lr_yhat0 = lr.predict(X0_test)


#Model1
lr.fit(X1_train, y1_train)
lr_yhat1 = lr.predict(X1_test)

#Model2
lr.fit(X2_train, y2_train)
lr_yhat2 = lr.predict(X2_test)


#4. SVM
svm=SVC()

#Model0
svm.fit(X0_train, y0_train)
svm_yhat0 = svm.predict(X0_test)


#Model1
svm.fit(X1_train, y1_train)
svm_yhat1= svm.predict(X1_test)

#Model2
svm.fit(X2_train, y2_train)
svm_yhat2 = svm.predict(X2_test)

# 5. Random Forest Tree

rf = RandomForestClassifier(max_depth = 4)

#Model0
rf.fit(X0_train, y0_train)
rf_yhat0 = rf.predict(X0_test)

#Model1
rf.fit(X1_train, y1_train)
rf_yhat1 = rf.predict(X1_test)

#Model0
rf.fit(X2_train, y2_train)
rf_yhat2 = rf.predict(X2_test)

# 1. Accuracy score

print('ACCURACY SCORE')
print('------------------------------------------------------------------------',)
print('Accuracy score of the Decision Tree model is {}'.format(np.mean([accuracy_score(y0_test, tree_yhat0),accuracy_score(y1_test, tree_yhat1),accuracy_score(y2_test, tree_yhat2)])))
print('------------------------------------------------------------------------')
print('Accuracy score of the KNN model is {}'.format(np.mean([accuracy_score(y0_test, knn_yhat0),accuracy_score(y1_test, knn_yhat1),accuracy_score(y2_test, knn_yhat2)])))
print('------------------------------------------------------------------------')
print('Accuracy score of the Logistic Regression model is {}'.format(np.mean([accuracy_score(y0_test, lr_yhat0),accuracy_score(y1_test, lr_yhat1),accuracy_score(y2_test, lr_yhat2)])))
print('------------------------------------------------------------------------')
print('Accuracy score of the SVM model is {}'.format(np.mean([accuracy_score(y0_test, svm_yhat0),accuracy_score(y1_test, svm_yhat1),accuracy_score(y2_test, svm_yhat2)])))
print('------------------------------------------------------------------------')
print('Accuracy score of the Random Forest Tree model is {}'.format(np.mean([accuracy_score(y0_test, rf_yhat0),accuracy_score(y1_test, rf_yhat1),accuracy_score(y2_test, rf_yhat2)])))
print('------------------------------------------------------------------------')

# 2. F1 score

print('F1 SCORE')
print('------------------------------------------------------------------------')
print('F1 score of the Decision Tree model is {}'.format(np.mean([f1_score(y0_test, tree_yhat0),f1_score(y1_test, tree_yhat1),f1_score(y2_test, tree_yhat2)])))
print('------------------------------------------------------------------------')
print('F1 score of the KNN model is {}'.format(np.mean([f1_score(y0_test, knn_yhat0),f1_score(y1_test, knn_yhat1),f1_score(y2_test, knn_yhat2)])))
print('------------------------------------------------------------------------')
print('F1 score of the Logistic Regression model is {}'.format(np.mean([f1_score(y0_test, lr_yhat0),f1_score(y1_test, lr_yhat1),f1_score(y2_test, lr_yhat2)])))
print('------------------------------------------------------------------------')
print('F1 score of the SVM model is {}'.format(np.mean([f1_score(y0_test, svm_yhat0),f1_score(y1_test, svm_yhat1),f1_score(y2_test, svm_yhat2)])))
print('------------------------------------------------------------------------')
print('F1 score of the Random Forest Tree model is {}'.format(np.mean([f1_score(y0_test, rf_yhat0),f1_score(y1_test, rf_yhat1),f1_score(y2_test, rf_yhat2)])))
print('------------------------------------------------------------------------')


# 4. Precision Score

print('Precision Score')
print('------------------------------------------------------------------------')
print('Precision score of the Decision Tree model is {}'.format(np.mean([precision_score(y0_test, tree_yhat0),precision_score(y1_test, tree_yhat1),precision_score(y2_test, tree_yhat2)])))
print('------------------------------------------------------------------------')
print('Precision score of the KNN model is {}'.format(np.mean([precision_score(y0_test, knn_yhat0),precision_score(y1_test, knn_yhat1),precision_score(y2_test, knn_yhat2)])))
print('------------------------------------------------------------------------')
print('Precision score of the Logistic Regression model is {}'.format(np.mean([precision_score(y0_test, lr_yhat0),precision_score(y1_test, lr_yhat1),precision_score(y2_test, lr_yhat2)])))
print('------------------------------------------------------------------------')
print('Precision score of the SVM model is {}'.format(np.mean([precision_score(y0_test, svm_yhat0),precision_score(y1_test, svm_yhat1),precision_score(y2_test, svm_yhat2)])))
print('------------------------------------------------------------------------')
print('Precision score of the Random Forest Tree model is {}'.format(np.mean([precision_score(y0_test, rf_yhat0),precision_score(y1_test, rf_yhat1),precision_score(y2_test, rf_yhat2)])))
print('------------------------------------------------------------------------')

# 5. Recall Score

print('Recall Score')
print('------------------------------------------------------------------------')
print('Recall score of the Decision Tree model is {}'.format(np.mean([recall_score(y0_test, tree_yhat0),recall_score(y1_test, tree_yhat1),recall_score(y2_test, tree_yhat2)])))
print('------------------------------------------------------------------------')
print('Recall score of the KNN model is {}'.format(np.mean([recall_score(y0_test, knn_yhat0),recall_score(y1_test, knn_yhat1),recall_score(y2_test, knn_yhat2)])))
print('------------------------------------------------------------------------')
print('Recall score of the Logistic Regression model is {}'.format(np.mean([recall_score(y0_test, lr_yhat0),recall_score(y1_test, lr_yhat1),recall_score(y2_test, lr_yhat2)])))
print('------------------------------------------------------------------------')
print('Recall score of the SVM model is {}'.format(np.mean([recall_score(y0_test, svm_yhat0),recall_score(y1_test, svm_yhat1),recall_score(y2_test, svm_yhat2)])))
print('------------------------------------------------------------------------')
print('Recall score of the Random Forest Tree model is {}'.format(np.mean([recall_score(y0_test, rf_yhat0),recall_score(y1_test, rf_yhat1),recall_score(y2_test, rf_yhat2)])))
print('------------------------------------------------------------------------')

# 3. confusion matrix

tree_matrix = confusion_matrix(y_test, tree_yhat, labels = [0, 1]) # Decision Tree
knn_matrix = confusion_matrix(y_test, knn_yhat, labels = [0, 1]) # K-Nearest Neighbors
lr_matrix = confusion_matrix(y_test, lr_yhat, labels = [0, 1]) # Logistic Regression
svm_matrix = confusion_matrix(y_test, svm_yhat, labels = [0, 1]) # Support Vector Machine
rf_matrix = confusion_matrix(y_test, rf_yhat, labels = [0, 1]) # Random Forest Tree
print('------------------------------------------------------------------------')
print("tree_matrix")
print(tree_matrix)
print('------------------------------------------------------------------------')
print("knn_matrix")
print(knn_matrix)
print('------------------------------------------------------------------------')
print("lr_matrix")
print(lr_matrix)
print('------------------------------------------------------------------------')
print("svm_matrix")
print(svm_matrix)
print('------------------------------------------------------------------------')
print("rf_matrix")
print(rf_matrix)
print('------------------------------------------------------------------------')

# 6. classification report

print('classification report')
print('------------------------------------------------------------------------')
print('classification report of the Decision Tree model is {}'.format(classification_report(y_test, tree_yhat)))
print('------------------------------------------------------------------------')
print('classification reportof the KNN model is {}'.format(classification_report(y_test, knn_yhat)))
print('------------------------------------------------------------------------')
print('classification report of the Logistic Regression model is {}'.format(classification_report(y_test, lr_yhat)))
print('------------------------------------------------------------------------')
print('classification report of the SVM model is {}'.format(classification_report(y_test, svm_yhat)))
print('------------------------------------------------------------------------')
print('classification report of the Random Forest Tree model is {}'.format(classification_report(y_test, rf_yhat)))
print('------------------------------------------------------------------------')